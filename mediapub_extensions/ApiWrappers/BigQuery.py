import uuid
import time
import datetime
from google.cloud import bigquery
import google.cloud.bigquery.job
from google.cloud import storage
import os
import json

class BigQuery(object):
    """
    Handles connections and queries to Google BigQuery

    This class handles connections to Google BigQuery and allows queries to be run on it.

    Requires:
        google-api-python-client: pip install google-api-python-client
        google-cloud: pip install google-cloud==0.27.0

    Attributes:
        client (google.cloud.bigquery.Client): The BigQuery connection
        project (str): The Google Cloud project
        verbose (bool): The verbosity flag
    """
    client = None
    verbose = False
    project = None
    credentials = None

    # dataset = None
    # storageClient = None

    def __init__(self, cred_file, project, verbose=False):
        """
        Create a BigQuery connection

        Args:
            project (str): The Google Cloud project to connect to

        Yields:
            google.cloud.bigquery.Client: A BigQuery connection.
        """
        self.verbose = verbose
        self.project = project
        self.credentials = cred_file
        if self.verbose: print('initializing BigQuery client...')
        #NOTE: The Client only takes specific Google data formats, the from_service_account_json generates the proper format from a json file
        #NOTE: I am not sure if this project param does anything
        self.client = bigquery.Client.from_service_account_json(self.credentials, project=self.project)

    def run_query(self, query, use_legacy_sql=False, query_id=str(uuid.uuid4()), use_query_cache=True ,destination_dataset=None, destination_table=None, truncate=False):
        """
        Run a query

        Args:
            query (str): The query to be run
            use_legacy_sql (bool): Is the input query written using legacy sql
            query_id (str): The unique ID for the query
            use_query_cache (bool): Should the query use cached data when available
            destination_dataset (str): The dataset the destination_table should be saved to.
            destination_table (str): If the data should be saved to a flat table, name it here.
            truncate (bool): Should the destination_table be truncated before writing new rows.

        Returns:
            google.cloud.bigquery.query.QueryResults: The unprocessed results of the query
        """
        job = self.client.run_async_query(query_id, query)
        job.use_query_cache = use_query_cache
        job.use_legacy_sql = use_legacy_sql

        if destination_table is not None:
            #NOTE: This is always set to true now, why would we ever not want large results, can be parameterized
            job.allow_large_results = True
            self.dataset = self.client.dataset(datasetIn) #NOTE: This saved to the class before, is that still needed?
            job.destination = self.dataset.table(destination_table)
            job.create_disposition = (google.cloud.bigquery.job.CreateDisposition.CREATE_IF_NEEDED) #NOTE: This might need to be parameterized
            if truncate:
                job.write_disposition = (google.cloud.bigquery.job.WriteDisposition.WRITE_TRUNCATE)
            else:
                job.write_disposition = (google.cloud.bigquery.job.WriteDisposition.WRITE_APPEND)
        if self.verbose: print("Starting Query")
        job.begin()
        self.wait_for_job(job)
        results = job.query_results()
        return results

    def wait_for_job(self,job):
        """ Blocking poll for query status """
        if self.verbose: print("Waiting for results...")
        while True:
            job.reload()
            # Retry the job every second until it finishes.  Throw an exception if it fails.
            if job.state == 'DONE':
                if job.error_result:
                    raise RuntimeError(job.errors)
                return
            time.sleep(1)

    def process_results(self, results, max_results=1000, fetch_all=True):
        """
        Process the results from a QueryResults object

        Allows the results of a QueryResults object to be looped through and return the data

        Args:
            results (google.cloud.bigquery.query.QueryResults): The results of the query
            max_results (int): The maximum number of results to return on each page
            fetch_all (bool): Should all of the results across the pages be returned

        Returns:
            data (list): a list of the returned results
            tot (int): The number of rows processed
        """

        #NOTE: The page token in this api version is a key generated by Google, so atm, the user cannot request a specific page of results
        page_token = None
        cols = []
        data = []
        rows = results.fetch_data(max_results=max_results, page_token=page_token)

        while fetch_all:
            if self.verbose: print("Fetching next page...")
            row_dat = list(rows)
            for row in row_dat:
                data.append(row)
            #TODO: This makes a list of lists, where each inner list is a page of results... stop that!
            if rows.next_page_token is None: # Stop looping on the last page of results
                break
            rows = results.fetch_data(max_results=max_results, page_token=rows.next_page_token)

        schema = results.schema
        for col in schema:
            cols.append(col.name)

        tot = results.total_rows
        if self.verbose: print("Processed " + str(tot) + " results")
        return data, cols, tot

if __name__ == '__main__':
    bq = BigQuery("C:\\BigQuery-422148a82d7c.json", 'feisty-gateway-727', verbose=True)
    results = bq.run_query("SELECT * FROM `feisty-gateway-727.stAdHoc.ap_entitlements` LIMIT 10")
    data, schema, tot = bq.process_results(results)
    # print(data)
    # print(schema)
