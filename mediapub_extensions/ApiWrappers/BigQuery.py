import uuid
import time
import datetime
from google.cloud import bigquery
import google.cloud.bigquery.job
from google.cloud.bigquery.job import DestinationFormat
from google.cloud import storage
import os
import json

class BigQuery(object):
    """
    Handles connections and queries to Google BigQuery

    This class handles connections to Google BigQuery and allows queries to be run on it.

    Requires:
        google-api-python-client: pip install google-api-python-client
        google-cloud: pip install google-cloud==0.27.0

    Attributes:
        client (google.cloud.bigquery.Client): The BigQuery connection
        project (str): The Google Cloud project
        verbose (bool): The verbosity flag
    """
    client = None
    verbose = False
    project = None
    credentials = None

    # dataset = None
    # storageClient = None

    def __init__(self, cred_file, project, verbose=False):
        """
        Create a BigQuery connection

        Args:
            project (str): The Google Cloud project to connect to

        Yields:
            google.cloud.bigquery.Client: A BigQuery connection.
        """
        self.verbose = verbose
        self.project = project
        self.credentials = cred_file
        if self.verbose: print('initializing BigQuery client...')
        #NOTE: The Client only takes specific Google data formats, the from_service_account_json generates the proper format from a json file
        #NOTE: I am not sure if this project param does anything
        self.client = bigquery.Client.from_service_account_json(self.credentials, project=self.project)

    def run_query(self, query, use_legacy_sql=False, query_id=str(uuid.uuid4()), use_query_cache=True ,destination_dataset=None, destination_table=None, truncate=False):
        """
        Run a query

        Args:
            query (str): The query to be run
            use_legacy_sql (bool): Is the input query written using legacy sql
            query_id (str): The unique ID for the query
            use_query_cache (bool): Should the query use cached data when available
            destination_dataset (str): The dataset the destination_table should be saved to.
            destination_table (str): If the data should be saved to a flat table, name it here.
            truncate (bool): Should the destination_table be truncated before writing new rows.

        Returns:
            google.cloud.bigquery.query.QueryResults: The unprocessed results of the query
        """
        job = self.client.run_async_query(query_id, query)
        job.use_query_cache = use_query_cache
        job.use_legacy_sql = use_legacy_sql

        if destination_table is not None:
            #NOTE: This is always set to true now, why would we ever not want large results, can be parameterized
            job.allow_large_results = True
            self.dataset = self.client.dataset(destination_dataset) #NOTE: This saved to the class before, is that still needed?
            job.destination = self.dataset.table(destination_table)
            job.create_disposition = (google.cloud.bigquery.job.CreateDisposition.CREATE_IF_NEEDED) #NOTE: This might need to be parameterized
            if truncate:
                job.write_disposition = (google.cloud.bigquery.job.WriteDisposition.WRITE_TRUNCATE)
            else:
                job.write_disposition = (google.cloud.bigquery.job.WriteDisposition.WRITE_APPEND)
        if self.verbose: print("Starting Query")
        job.begin()
        self.wait_for_job(job)
        results = job.query_results()
        return results

    def wait_for_job(self,job):
        """ Blocking poll for query status """

        if self.verbose: print("Waiting for results...")
        while True:
            job.reload()
            # Retry the job every second until it finishes.  Throw an exception if it fails.
            if job.state == 'DONE':
                if job.error_result:
                    raise RuntimeError(job.errors)
                return
            time.sleep(1)

    def process_results(self, results, max_results=1000, fetch_all=True):
        """
        Process the results from a QueryResults object

        Allows the results of a QueryResults object to be looped through and return the data

        Args:
            results (google.cloud.bigquery.query.QueryResults): The results of the query
            max_results (int): The maximum number of results to return on each page
            fetch_all (bool): Should all of the results across the pages be returned

        Returns:
            data (list): a list of the returned results
            tot (int): The number of rows processed
        """

        #NOTE: The page token in this api version is a key generated by Google, so atm, the user cannot request a specific page of results
        page_token = None
        cols = []
        data = []
        rows = results.fetch_data(max_results=max_results, page_token=page_token)

        while fetch_all:
            if self.verbose: print("Fetching next page...")
            row_dat = list(rows)
            for row in row_dat:
                data.append(row)
            #TODO: This makes a list of lists, where each inner list is a page of results... stop that!
            if rows.next_page_token is None: # Stop looping on the last page of results
                break
            rows = results.fetch_data(max_results=max_results, page_token=rows.next_page_token)

        schema = results.schema
        for col in schema:
            cols.append(col.name)

        tot = results.total_rows
        if self.verbose: print("Processed " + str(tot) + " results")
        return data, cols, tot

    def export_table(self, project, dataset, table_id, bucket, filename, format="NEWLINE_DELIMITED_JSON"):
        """
        Saves a table to GCS

        Save the data from Google BigQuery to Google Cloud Storage (GCS)

        Args:
            project (str): The GCP Project name
            dataset (str): The name of the dataset containing the table in question
            table_id (str): The table name
            bucket (str): The GCS bucket location (i.e. folder)
            filename (str): The desired filename of the output files
            format (str): The format of the resulting file

        Returns:
            job_id (str): The ID of the job
            result (str): The final state of the job
        """

        destination_url = "gs://{}/{}".format(bucket, filename)
        job_id = str(uuid.uuid4())
        table_ref = self.client.dataset(dataset, project=project).table(table_id)
        if self.verbose: print("Starting load of {} to {} as {}".format(table_id, destination_url, job_id))

        extract_job = self.client.extract_table_to_storage(job_id, table_ref, destination_url)
        extract_job.destination_format = format
        extract_job._build_resource()
        extract_job.begin()
        result = extract_job.result().state
        if self.verbose: print("Job {} is finished with a status of {}".format(job_id, result))
        return job_id, result

    def download_export(self, bucket, filename, destination_filename=None):
        if not destination_filename: destination_filename = filename
        storage_client = storage.Client.from_service_account_json(self.credentials, project=self.project)
        bucket = storage_client.get_bucket(bucket)
        bucket.blob(filename).download_to_filename(destination_filename)

    def get_gcs_files(self, bucket):
        storage_client = storage.Client.from_service_account_json(self.credentials, project=self.project)
        bucket = storage_client.get_bucket(bucket)
        return [b.name for b in bucket.list_blobs()]

    def delete_gcs_files(self, bucket, file):
        storage_client = storage.Client.from_service_account_json(self.credentials, project=self.project)
        bucket = storage_client.get_bucket(bucket)
        blob = bucket.blob(file)
        blob.delete()

    def delete_gcs_files_in_bucket(self, bucket):
        storage_client = storage.Client.from_service_account_json(self.credentials, project=self.project)
        bucket = storage_client.get_bucket(bucket)
        for b in bucket.list_blobs():
            blob = bucket.blob(b.name)
            blob.delete()

if __name__ == '__main__':
    print("Don't call directly.  Install package and import as a class.")
